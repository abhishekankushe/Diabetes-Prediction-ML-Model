{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfefeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Decision Tree on Diabetes Dataset\n",
    "# =========================\n",
    "\n",
    "# 1) Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    balanced_accuracy_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a397bd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2) Load data\n",
    "# If running in the same folder as the attached file, set:\n",
    "csv_path = Path(\"diabetes_dataset.csv\")  # change to the actual path if needed\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "print(\"Shape:\", df.shape)\n",
    "print(df.head(3))\n",
    "\n",
    "# 3) Basic checks\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df.isna().sum())\n",
    "\n",
    "print(\"\\nOutcome distribution:\")\n",
    "print(df['Outcome'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177ce75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4) Handle likely invalid zeros in medical features\n",
    "# In the Pima dataset, zeros in these columns often mean \"missing\":\n",
    "zero_as_missing_cols = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']\n",
    "\n",
    "df_clean = df.copy()\n",
    "for col in zero_as_missing_cols:\n",
    "    # Replace zeros with NaN\n",
    "    df_clean[col] = df_clean[col].replace(0, np.nan)\n",
    "    # Impute with median (robust to outliers)\n",
    "    median_val = df_clean[col].median()\n",
    "    df_clean[col] = df_clean[col].fillna(median_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5e561a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Split features and target\n",
    "X = df_clean.drop(columns=['Outcome'])\n",
    "y = df_clean['Outcome'].astype(int)\n",
    "\n",
    "# 6) Train/test split (stratified to preserve class ratio)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=41, stratify=y\n",
    ")\n",
    "\n",
    "print(\"\\nTrain shape:\", X_train.shape, \"Test shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f63d65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Optional scaling (trees donâ€™t need it, but harmless and sometimes helps with stability)\n",
    "# Comment out if you prefer raw features.\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ca9429",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# 8) Baseline Decision Tree (quick start)\n",
    "baseline_clf = DecisionTreeClassifier(random_state=41)\n",
    "baseline_clf.fit(X_train_scaled, y_train)\n",
    "baseline_pred = baseline_clf.predict(X_test_scaled)\n",
    "\n",
    "print(\"\\n=== Baseline Decision Tree ===\")\n",
    "print(\"Accuracy:\", f\"{accuracy_score(y_test, baseline_pred):.4f}\")\n",
    "print(\"Balanced Accuracy:\", f\"{balanced_accuracy_score(y_test, baseline_pred):.4f}\")\n",
    "print(\"Precision (pos=1):\", f\"{precision_score(y_test, baseline_pred, zero_division=0):.4f}\")\n",
    "print(\"Recall (pos=1):\", f\"{recall_score(y_test, baseline_pred, zero_division=0):.4f}\")\n",
    "print(\"F1 (pos=1):\", f\"{f1_score(y_test, baseline_pred, zero_division=0):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fd858f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 9) Hyperparameter tuning with GridSearchCV for better generalization\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy', 'log_loss'],  # log_loss uses information gain with log loss\n",
    "    'max_depth': [None, 3, 4, 5, 6, 8, 10],\n",
    "    'min_samples_split': [2, 5, 10, 20],\n",
    "    'min_samples_leaf': [1, 2, 4, 8],\n",
    "    'max_features': [None, 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=41)\n",
    "grid = GridSearchCV(\n",
    "    DecisionTreeClassifier(random_state=41),\n",
    "    param_grid=param_grid,\n",
    "    scoring='balanced_accuracy',\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "best_clf = grid.best_estimator_\n",
    "print(\"\\n=== Best CV Model ===\")\n",
    "print(\"Best params:\", grid.best_params_)\n",
    "print(\"Best CV balanced accuracy:\", f\"{grid.best_score_: .4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13748dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10) Evaluate the tuned model\n",
    "y_pred = best_clf.predict(X_test_scaled)\n",
    "print(\"\\n=== Test Performance (Tuned Model) ===\")\n",
    "print(\"Accuracy:\", f\"{accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(\"Balanced Accuracy:\", f\"{balanced_accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(\"Precision (pos=1):\", f\"{precision_score(y_test, y_pred, zero_division=0):.4f}\")\n",
    "print(\"Recall (pos=1):\", f\"{recall_score(y_test, y_pred, zero_division=0):.4f}\")\n",
    "print(\"F1 (pos=1):\", f\"{f1_score(y_test, y_pred, zero_division=0):.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred, target_names=['No Diabetes (0)','Diabetes (1)'], digits=4, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cb8ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11) Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred, labels=[0,1])\n",
    "print(\"Confusion Matrix (rows=true, cols=pred) [0=No, 1=Yes]:\\n\", cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573eefab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract TN, FP, FN, TP\n",
    "TN, FP, FN, TP = cm.ravel()\n",
    "print(f\"TN: {TN}, FP: {FP}, FN: {FN}, TP: {TP}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f07ac92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot heatmap\n",
    "plt.figure(figsize=(4,3))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "            xticklabels=['No (0)','Yes (1)'], yticklabels=['No (0)','Yes (1)'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac651bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12) Feature importances\n",
    "importances = pd.Series(best_clf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "print(\"\\nFeature Importances:\")\n",
    "print(importances)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.barplot(x=importances.values, y=importances.index, color='teal')\n",
    "plt.title('Decision Tree Feature Importances')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79eaae56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 13) Optional: visualize the tree (small trees only; may be large)\n",
    "# If the tree is too big, set smaller max_depth in best_clf or plot the baseline_clf with smaller depth.\n",
    "try:\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    plot_tree(best_clf, feature_names=X.columns, class_names=['No (0)','Yes (1)'],\n",
    "              filled=True, max_depth=3)  # limit depth in visualization for readability\n",
    "    plt.title(\"Decision Tree (Top Levels)\")\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(\"Tree plotting skipped:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e5aff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14) Build a production-ready pipeline and save it\n",
    "import joblib\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# Columns where 0 means missing\n",
    "zero_as_missing_cols = ['Glucose','BloodPressure','SkinThickness','Insulin','BMI']\n",
    "all_features = X.columns.tolist()\n",
    "\n",
    "def zeros_to_nan(df):\n",
    "    df = df.copy()\n",
    "    for c in zero_as_missing_cols:\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].replace(0, np.nan)\n",
    "    return df\n",
    "\n",
    "# Preprocess: 0->NaN, median impute, scale (kept because you trained with StandardScaler)\n",
    "preprocess = Pipeline(steps=[\n",
    "    ('zero_to_nan', FunctionTransformer(zeros_to_nan, validate=False)),\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())  # keep only if you used scaling in training\n",
    "])\n",
    "\n",
    "prod_pipe = Pipeline(steps=[\n",
    "    ('prep', preprocess),\n",
    "    ('clf', best_clf)  # your tuned DecisionTreeClassifier\n",
    "])\n",
    "\n",
    "# Fit the pipeline fully on the training data (X_train, y_train)\n",
    "prod_pipe.fit(X_train, y_train)\n",
    "\n",
    "# Save the pipeline\n",
    "joblib.dump(prod_pipe, 'diabetes_decision_tree_pipeline.joblib')\n",
    "print(\"Saved pipeline to diabetes_decision_tree_pipeline.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daec8960",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b85a88f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
